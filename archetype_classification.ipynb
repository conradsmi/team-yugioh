{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be00ff03-8f2f-46be-9f40-73fc1db944e5",
   "metadata": {},
   "source": [
    "# YugiohNet\n",
    "\n",
    "### Yugioh Archetype Classification\n",
    "\n",
    "### Created by Conrad Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258526f0-bf08-442d-b147-a07676d7f2af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image, ImageFile\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LambdaCallback, CSVLogger\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, \\\n",
    "                         BatchNormalization, Conv2D, MaxPooling2D, ReLU, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "# ensures GPU doesn't crash mid training\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# necessary for creating dataset\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894560c-2ecc-4caa-839e-e41acb6986db",
   "metadata": {},
   "source": [
    "## Global Environment\n",
    "\n",
    "#### Hyperparameter / Constants Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c23f30fa-35b9-4c61-90d2-4c3b64303573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GlobalEnvironment:\n",
    "    def __init__(self, \n",
    "                 data_url: str, \n",
    "                 data_path: str, \n",
    "                 data_file_name: str,\n",
    "                 experiment_name: str,\n",
    "                 load_cp: bool = False,\n",
    "                 load_cp_and_train: bool = True,\n",
    "                 load_model: bool = False,\n",
    "                 seed: int = 1,\n",
    "                 image_size = (224, 224)):\n",
    "        self.data_url = data_url\n",
    "        self.data_path = data_path\n",
    "        self.data_file_name = data_file_name\n",
    "        self.seed = seed\n",
    "        self.image_size = image_size\n",
    "        self.experiment_name = experiment_name\n",
    "        self.load_cp = load_cp\n",
    "        self.load_cp_and_train = load_cp_and_train\n",
    "        self.load_model = load_model\n",
    "        \n",
    "        self.data_file_path = os.path.join(data_path, data_file_name)\n",
    "        self.class_number = len(next(os.walk(self.data_path))[1])\n",
    "        \n",
    "        # parameter space for optuna\n",
    "        self.p_optuna = {\n",
    "            'activation': ('leakyrelu', 'relu'),\n",
    "            'optimizer': ('Adam', 'Nadam', 'RMSprop', 'SGD'),\n",
    "            'batch_size': (16, 32, 64),\n",
    "            'leaky_alpha': (0.001, 0.3),\n",
    "            'lr': (1e-5, 1e-1),\n",
    "            'epochs': 8\n",
    "        }\n",
    "        \n",
    "        # parameter space not involved in hyperparameter tuning\n",
    "        self.p_const = {\n",
    "            'activation': 'relu',\n",
    "            'optimizer': 'RMSProp',\n",
    "            'batch_size': 32,\n",
    "            'epochs': 200,\n",
    "            'leaky_alpha': 0.1,\n",
    "            'lr': 0.001\n",
    "        }\n",
    "\n",
    "        \n",
    "Global = GlobalEnvironment(experiment_name = 'vanilla',\n",
    "                           data_url = 'https://db.ygoprodeck.com/api/v7/cardinfo.php',\n",
    "                           data_path = 'data',\n",
    "                           data_file_name = 'archetypes.csv')\n",
    "np.random.seed(Global.seed)\n",
    "tf.random.set_seed(Global.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be9ca0-5ad4-4b53-a399-fbdf3af7d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(Global.data_url)\n",
    "data = r.json()\n",
    "pdata = json.dumps(data, indent=4)\n",
    "# Print the type of data variable\n",
    "print(\"Type:\", type(data))\n",
    "print(data['data'][0]['archetype'])\n",
    "\n",
    "# Print the data of dictionary\n",
    "print(\"\\nArchetype Card:\", json.dumps(data['data'][0], indent=4))\n",
    "print(\"\\nNon-Archetype Card:\", json.dumps(data['data'][8], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3bb37-7348-44bf-9917-77854f49aba7",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc602c1a-4ae8-48a5-873c-976d19cc3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(\"https://db.ygoprodeck.com/api/v7/cardinfo.php\")\n",
    "# data = r.json()\n",
    "def create_dataset():\n",
    "    cards = data['data']\n",
    "    n = len(cards)\n",
    "    prog = Progbar(n)\n",
    "    # iterate through every card; index needed for progress bar\n",
    "    for i in range(n):\n",
    "        card = cards[i]\n",
    "        \n",
    "        # some cards do not have an archetype field; label it as 'No_Archetype' (this will cause an error otherwise)\n",
    "        # also, some archetypes have slashes which is problematic. replace those with underscores\n",
    "        archetype_path = os.path.join(Global.data_path, (card['archetype'] if 'archetype' in card else 'No_Archetype').replace(\"/\", \"_\"))\n",
    "        # make directory if archetype doesn't exist\n",
    "        if not os.path.isdir(archetype_path):\n",
    "            os.mkdir(archetype_path)\n",
    "            \n",
    "        for image in card['card_images']:\n",
    "            file_path = os.path.join(archetype_path, '{}.png'.format(image['id']))\n",
    "            # if we've fetched the image for this card ID previously, skip it\n",
    "            if not os.path.exists(file_path):\n",
    "                url = image['image_url']\n",
    "                # fetch raw picture content from URL\n",
    "                # https://stackoverflow.com/a/18043472\n",
    "                response = requests.get(url, stream=True)\n",
    "                with open(file_path, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "                    Image.open(file_path).resize(Global.image_size).save(file_path)\n",
    "        \n",
    "        # update progress bar so we know how long this is going to take\n",
    "        prog.update(i)\n",
    "    # finish progress bar\n",
    "    prog.update(n, finalize=True)\n",
    "    \n",
    "# create the CSV\n",
    "def generate_csv():\n",
    "    cards = pd.DataFrame(data['data'])\n",
    "    print(cards.loc[0, ['archetype']])\n",
    "    cards['archetype'] = cards['archetype'].fillna('No_Archetype')\n",
    "    # cards = cards.loc[:, ['card_images'[:, 'id'], 'archetype']]\n",
    "    # cards = cards['id', 'archetype']\n",
    "    entries = []\n",
    "    for i in range(len(cards)):\n",
    "        card = cards.loc[i]\n",
    "        for j in range(len(card.loc['card_images'])):\n",
    "            image = card.loc['card_images'][j]\n",
    "        entries.append([os.path.join(Global.data_path, card['archetype'], '{}.png'.format(image['id'])), image['id'], card['archetype']])\n",
    "\n",
    "    entries = pd.DataFrame(entries)\n",
    "    for i in range(10):\n",
    "        print(entries.iloc[i])\n",
    "    entries.columns = ['file_path', 'id', 'archetype']\n",
    "    entries.to_csv(Global.data_file_path, index=False)\n",
    "\n",
    "def generate_freqs():\n",
    "    freqs = pd.read_csv(Global.data_file_path, delimiter=',').value_counts(subset=['archetype'])\n",
    "    # freqs = pd.DataFrame(freqs)\n",
    "    print(freqs[:10])\n",
    "    freqs.plot(ylim=(0, freqs[1]))\n",
    "    \n",
    "def resize_images():\n",
    "    cards = data['data']\n",
    "    n = len(cards)\n",
    "    prog = Progbar(n)\n",
    "    # iterate through every card; index needed for progress bar\n",
    "    for i in range(n):\n",
    "        card = cards[i]\n",
    "        \n",
    "        # some cards do not have an archetype field; label it as 'No_Archetype' (this will cause an error otherwise)\n",
    "        # also, some archetypes have slashes which is problematic. replace those with underscores\n",
    "        archetype_path = os.path.join(Global.data_path, (card['archetype'] if 'archetype' in card else 'No_Archetype').replace(\"/\", \"_\"))\n",
    "        # make directory if archetype doesn't exist\n",
    "        if not os.path.isdir(archetype_path):\n",
    "            os.mkdir(archetype_path)\n",
    "            \n",
    "        for image in card['card_images']:\n",
    "            file_path = os.path.join(archetype_path, '{}.png'.format(image['id']))\n",
    "            # if we've fetched the image for this card ID previously, skip it\n",
    "            if os.path.exists(file_path):\n",
    "                Image.open(file_path).resize(Global.image_size).save(file_path)\n",
    "        \n",
    "        # update progress bar so we know how long this is going to take\n",
    "        prog.update(i)\n",
    "    # finish progress bar\n",
    "    prog.update(n, finalize=True)\n",
    "        \n",
    "# create_dataset()\n",
    "# generate_csv()\n",
    "generate_freqs()\n",
    "# resize_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca16b91-3c7e-49d9-a867-7310256ee0f5",
   "metadata": {},
   "source": [
    "## TF Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e74b0-38c7-4156-b9fe-5b4d8b118227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_generator, batch_size, subset):\n",
    "    generator = data_generator.flow_from_directory(\n",
    "        directory = Global.data_path,\n",
    "        target_size = Global.image_size,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        seed = Global.seed,\n",
    "        subset = subset\n",
    "    )\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc921e92-e502-49b6-8796-7c1fc8b24e01",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning / Fine Tuning / Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03643efb-af2a-48f5-b1cb-9c35ded3a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "   # ---------- Model Hyperparameters ----------\n",
    "    batch_size = Global.p_const['batch_size']\n",
    "    # dropout = Global.p_const['dropout']\n",
    "    lr = Global.p_const['lr']\n",
    "    optim = tf.keras.optimizers.get({\"class_name\": Global.p_const['optimizer'], \n",
    "                                     \"config\": {\"learning_rate\": lr}})\n",
    "    activation_layer = LeakyReLU(alpha = Global.p_const['leaky_alpha']) if Global.p_const['activation'] == 'leakyrelu' else ReLU()\n",
    "    \n",
    "    # if conducting an optuna study\n",
    "    if trial != None:\n",
    "        batch_size = trial.suggest_categorical('batch_size', Global.p_optuna['batch_size'])\n",
    "        lr = trial.suggest_float('lr', Global.p_optuna['lr'][0], Global.p_optuna['lr'][1])\n",
    "        optim = tf.keras.optimizers.get({\"class_name\": trial.suggest_categorical('optimizer', Global.p_optuna['optimizer']), \n",
    "                                         \"config\": {\"learning_rate\": lr}})\n",
    "\n",
    "        activation_layer = trial.suggest_categorical('activation', Global.p_optuna['activation'])\n",
    "        if activation_layer == 'leakyrelu':\n",
    "            activation_layer = LeakyReLU(alpha = trial.suggest_float('leaky_alpha', Global.p_optuna['leaky_alpha'][0], Global.p_optuna['leaky_alpha'][1]))\n",
    "        else:\n",
    "            activation_layer = ReLU()\n",
    "    \n",
    "    # ---------- Model Architecture ----------\n",
    "    model = Sequential()\n",
    "    if Global.load_model:\n",
    "        # skip training loop and just return model\n",
    "        model = tf.keras.models.load_model('./models/' + Global.experiment_name, compile = False)\n",
    "        return model\n",
    "    else:\n",
    "        resnet = ResNet50(\n",
    "            include_top = False, \n",
    "            weights = None, \n",
    "            input_shape = (*Global.image_size, 3), \n",
    "            pooling = 'max'\n",
    "        )\n",
    "        for l in resnet.layers:\n",
    "            l.trainable = True\n",
    "        model.add(resnet)\n",
    "        # model.add(Flatten(input_shape = resnet.output_shape[1:]))\n",
    "        model.add(Dense(\n",
    "            Global.class_number, \n",
    "            kernel_initializer=GlorotUniform(seed = Global.seed),\n",
    "            activation='softmax'))\n",
    "\n",
    "        # compile \n",
    "\n",
    "    model.compile(\n",
    "        optimizer = optim, \n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "        metrics = [tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    # ---------- Callbacks ----------\n",
    "    # set up checkpoints\n",
    "    # https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    checkpoint_path = \"experiments/cp.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath = checkpoint_path,\n",
    "        save_weights_only = True,     \n",
    "        save_best_only = True,\n",
    "        verbose = 1\n",
    "    )\n",
    "    early_stopper = EarlyStopping(\n",
    "        monitor = 'val_loss',\n",
    "        min_delta = 0,\n",
    "        patience = 20, \n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        restore_best_weights = True\n",
    "    )\n",
    "    lr_reducer = ReduceLROnPlateau(\n",
    "        monitor = 'val_loss',\n",
    "        factor = 0.1,\n",
    "        patience = 10,\n",
    "        verbose = 1,\n",
    "        mode = 'min',\n",
    "        min_delta = 1e-3\n",
    "    )\n",
    "    csv_logger = CSVLogger(\n",
    "        filename = Global.experiment_name + '.csv',\n",
    "    )\n",
    "\n",
    "    # only want pruning callback if using optuna, as others aren't necessary for low epoch runs\n",
    "    callbacks = (cp_callback, early_stopper, lr_reducer, csv_logger) if trial is None else \\\n",
    "                (optuna.integration.TFKerasPruningCallback(trial, 'val_loss'))\n",
    "\n",
    "    # ---------- Loading, Training, Evaluation ----------\n",
    "    # don't need to train if just loading weights, but we did need to recompile the model\n",
    "    # in order to load weights\n",
    "    if Global.load_cp:\n",
    "        model.load_weights(checkpoint_path)\n",
    "        if Global.load_cp_and_train == False:\n",
    "            return model\n",
    "    \n",
    "    # train    \n",
    "    data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    train_generator = generate_data(data_generator, batch_size, subset='training')\n",
    "    valid_generator = generate_data(data_generator, batch_size, subset='validation')\n",
    "\n",
    "    if trial != None:\n",
    "        print('Trial Number: ', trial.number)\n",
    "        print('Trial Parameters: ', trial.params)\n",
    "    out = model.fit(\n",
    "        x = train_generator,\n",
    "        steps_per_epoch = train_generator.n // train_generator.batch_size, # for some reason tf won't do this automatically here\n",
    "        validation_data = valid_generator,\n",
    "        validation_steps = valid_generator.n // valid_generator.batch_size,\n",
    "        epochs = Global.p_const['epochs'] if trial is None else Global.p_optuna['epochs'],\n",
    "        callbacks = callbacks,\n",
    "        workers = 6,\n",
    "        verbose = 1\n",
    "    )\n",
    "\n",
    "    # evaluate\n",
    "    results = model.evaluate(\n",
    "        valid_generator, \n",
    "        verbose = 1, \n",
    "        use_multiprocessing = True,\n",
    "        steps = valid_generator.n // valid_generator.batch_size,\n",
    "        workers = 6\n",
    "    )\n",
    "    if trial is None:\n",
    "        model.save('./models/' + Global.experiment_name)\n",
    "    \n",
    "    print (results)\n",
    "    return results[0] if trial is not None else model # return validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e550e-0900-4500-bf06-5359300a395b",
   "metadata": {},
   "source": [
    "## Optuna Study Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6347147-8c81-4dce-87c1-0bb328c7cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some figures related to a completed optuna study\n",
    "def report(study):\n",
    "    fig = optuna.visualization.plot_optimization_history(study)\n",
    "    fig.show()\n",
    "    fig = optuna.visualization.plot_intermediate_values(study)\n",
    "    # fig.update_layout(yaxis_range=[0, 8100]) # remove outliers\n",
    "    fig.show()\n",
    "    fig = optuna.visualization.plot_contour(study, params = ['dropout', 'lr'])\n",
    "    fig.show()\n",
    "    fig = optuna.visualization.plot_param_importances(study)\n",
    "    fig.show() \n",
    "    \n",
    "    # https://github.com/optuna/optuna-examples/blob/main/tensorflow/tensorflow_eager_simple.py\n",
    "    trials = study.best_trials\n",
    "    for t in trials:\n",
    "        print(\"Trial: \", t.number, \", Validation Loss: \", t.value)\n",
    "        print(\"Params: \")\n",
    "        for key, value in t.params.items():\n",
    "            print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "# conduct an optuna study\n",
    "def conduct_study(study_name):\n",
    "    with tf.device('/device:GPU:0'): # utilizes the GPU of the remote server\n",
    "        store = 'sqlite:///' + study_name + '.db'\n",
    "        study = optuna.create_study(\n",
    "            study_name = study_name,\n",
    "            direction = 'minimize',\n",
    "            pruner = optuna.pruners.MedianPruner(),\n",
    "            storage = store\n",
    "        )\n",
    "        study.optimize(objective, n_trials = 100)\n",
    "        print('Done!')\n",
    "        \n",
    "# evaluate a completed optuna study\n",
    "def eval_study(study_name):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        store = 'sqlite:///' + study_name + '.db'\n",
    "        study = optuna.load_study(study_name = study_name,\n",
    "                                     storage = 'sqlite:///RISENet-SC-Study.db',\n",
    "                                     pruner = optuna.pruners.MedianPruner())\n",
    "        report(study)\n",
    "        print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135fd55-ab85-4a31-920b-c92f93b148af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tf.test.gpu_device_name():\n",
    "#     print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "# else:\n",
    "#     print(\"Please install GPU version of TF\")\n",
    "\n",
    "conduct_study(Global.experiment_name)\n",
    "#     model = objective(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718122f4-0526-4613-be09-a3f630cad9a7",
   "metadata": {},
   "source": [
    "## Load a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a15200-3b32-418f-96ce-360dd0b3a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model doesn't work with compile=True so I have to compile manually\n",
    "model = tf.keras.models.load_model('./models/' + Global.experiment_name, compile = False)\n",
    "model.compile(optimizer = tf.keras.optimizers.get({\"class_name\": Global.p_const['optimizer'], \n",
    "                                     \"config\": {\"learning_rate\": Global.p_const['lr']}}), \n",
    "              loss = M.circular_mse, \n",
    "              metrics = [M.circular_mae, M.mean_angular_error]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
